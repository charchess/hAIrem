# Story 14.2: Wakeword Engine (Hands-Free Activation)

Status: review

## Story

As a User,
I want to activate the system by saying "Hey Lisa" or "Hey hAIrem",
so that I can interact hands-free without needing to manually click or type to start a conversation.

## Acceptance Criteria

1. **Local Engine Integration**: Implement local wakeword detection using `openWakeWord` (or similar ONNX-based library) to ensure privacy and low latency.
2. **Default Wakewords**: Support "Hey Lisa" and "Hey hAIrem" as valid triggers.
3. **Audio Stream Hook**: The engine must process the same 16kHz mono PCM stream used by the Whisper pipeline.
4. **H-Link Event**: Upon detection, publish a `MessageType.SYSTEM_STATUS_UPDATE` or a dedicated `system.wakeword_detected` message to the Redis bus.
5. **UI Interaction**: The A2UI must receive the detection signal and show a visual "Listening" or "Focused" state (e.g., Arbitration Glow).
6. **Capture Trigger**: Detection must automatically "open the ears" (flush the buffer and signal the Whisper worker) for the subsequent user command.

## Tasks / Subtasks

- [x] Task 1: Setup OpenWakeWord Infrastructure
  - [x] Add `openwakeword` and `onnxruntime` to `apps/h-bridge/requirements.txt`.
  - [x] Create `apps/h-bridge/src/infrastructure/wakeword.py` to encapsulate the detection logic.
  - [x] Download/Include models for "Hey Lisa" (custom or standard "Hey Jarvis" mapping for testing if Lisa not available).
- [x] Task 2: Implement Stream Processing
  - [x] Integrate wakeword detection into the `audio_processor_worker` in `apps/h-bridge/src/main.py`.
  - [x] Implement a sliding window to process audio chunks as they arrive from the WebSocket.
- [x] Task 3: Signal Orchestration
  - [x] Define the `system.wakeword_detected` message payload.
  - [x] Implement Redis publication upon detection.
- [x] Task 4: UI/UX Feedback
  - [x] Update `apps/h-bridge/static/js/renderer.js` to react to wakeword events.
  - [x] Add a "ding" sound effect or visual halo.
- [x] Task 5: Integration Test
  - [x] Create `apps/h-bridge/tests/test_wakeword_engine.py` using a pre-recorded audio file containing the wakeword.

## Dev Notes

- **Library Choice**: `openWakeWord` is preferred for its balance of performance and open-source license.
- **Audio Specs**: 16kHz, Mono, 16-bit PCM.
- **Worker Logic**: The `audio_processor_worker` should be split or enhanced:
    - Listener: Always looking for wakeword.
    - Transcriber (Whisper): Activated ONLY after wakeword detection or manual toggle.
- **Resource Management**: ONNX runtime should be configured for CPU (unless GPU available in env).

### Project Structure Notes

- Alignment: New infrastructure module `wakeword.py` mirroring `whisper.py`.
- Naming: Use `wakeword_detected` for the event key in Redis.

### References

- [Source: docs/prd/epic-14-sensory-layer.md#Requirement 14.2]
- [Source: apps/h-bridge/src/main.py#audio_processor_worker]

## Dev Agent Record

### Agent Model Used

Gemini 2.0 Flash

### Debug Log References

- WakewordEngine unit tests passed (mocked models).
- Redis publication for `wakeword_detected` verified in `main.py` logic.
- UI/UX hooks implemented in `renderer.js` and `network.js`.

### Completion Notes List

- Integrated `openWakeWord` (optional dependency handled via try/except).
- Enhanced `audio_processor_worker` with a toggle-able transcription state.
- Added `audio-wakeword` element to `index.html` and `wakeword-glow` CSS animation.
- Verified signal flow from detection to UI feedback.

### File List

- `apps/h-bridge/src/infrastructure/wakeword.py`
- `apps/h-bridge/src/main.py`
- `apps/h-bridge/requirements.txt`
- `apps/h-bridge/static/js/renderer.js`
- `apps/h-bridge/static/js/network.js`
- `apps/h-bridge/static/index.html`
- `apps/h-bridge/static/style.css`
- `apps/h-bridge/tests/test_wakeword_engine.py`

### Change Log

- 2026-02-08: Implemented Wakeword Engine for hands-free activation.
