# Story 2.1: Pipeline Lazy Loading & State Feedback

Status: review

## Story

As a API consumer,
I want the pipeline to load models on-demand and report its loading state,
so that the API starts instantly and I know when the system is loading a model for the first time.

## Acceptance Criteria

1. **Given** the worker starts fresh (no model loaded) **When** the first generation request arrives **Then** the pipeline loads the requested model lazily (not at import time) **And** the Celery task state reports `"step": "model_loading"` during load

2. **Given** `compute_embedding()` is called on the pipeline **When** the function executes **Then** a `warnings.warn("Placeholder: embedding pre-computation not implemented")` is emitted **And** the function still returns its current placeholder value (`torch.zeros(1)`)

3. **Given** the pipeline has loaded a model **When** `is_healthy()` is called before a generation **Then** it returns `True` if the pipeline is in a consistent state **And** returns `False` if the pipeline is in a corrupted or partially loaded state

## Tasks / Subtasks

- [x] Task 1: Implement true lazy-loading in pipeline.py (AC: #1)
  - [x] 1.1 Remove `self.load_model(DEFAULT_MODEL)` from `FlexiblePipeline.__init__` (line 49)
  - [x] 1.2 Ensure `__init__` only initializes state variables (`self.pipe = None`, `self.current_model = None`, etc.)
  - [x] 1.3 Verify `generate()` already calls `self.load_model(model)` at line 302 — this becomes the first load trigger
  - [x] 1.4 Verify module-level `pipeline = FlexiblePipeline()` (line 381) no longer triggers model download at import
- [x] Task 2: Add model_loading state feedback in worker.py (AC: #1)
  - [x] 2.1 Add `self.update_state(state="PROGRESS", meta={"step": "model_loading", "progress": 5})` before `pipeline.generate()` when model is not yet loaded
  - [x] 2.2 Ensure state progression follows convention: `initialisation` → `model_loading` → `generation_gpu` → `sauvegarde` → `termine`
- [x] Task 3: Add warnings.warn to compute_embedding (AC: #2)
  - [x] 3.1 Add `import warnings` to pipeline.py
  - [x] 3.2 Add `warnings.warn("Placeholder: embedding pre-computation not implemented")` as first line in `compute_embedding()`
  - [x] 3.3 Keep the existing `return torch.zeros(1)` — do NOT change the return value
- [x] Task 4: Implement is_healthy() method (AC: #3)
  - [x] 4.1 Add `is_healthy()` method to `FlexiblePipeline` class
  - [x] 4.2 Return `True` when: `self._initialized is True` AND (`self.pipe is None` (no model loaded yet — valid idle state) OR (`self.pipe is not None` AND `self.compel is not None` AND `self.current_model is not None`))
  - [x] 4.3 Return `False` when: pipeline is in a partially loaded state (e.g., `self.pipe` set but `self.compel is None`, or `self.current_model` set but `self.pipe is None`)
- [x] Task 5: Write tests (AC: #1, #2, #3)
  - [x] 5.1 Test lazy-loading: importing pipeline module does NOT call `load_model`
  - [x] 5.2 Test lazy-loading: `FlexiblePipeline.__init__` does NOT call `load_model`
  - [x] 5.3 Test `compute_embedding()` emits `UserWarning` with correct message
  - [x] 5.4 Test `compute_embedding()` returns `torch.zeros(1)` (unchanged)
  - [x] 5.5 Test `is_healthy()` returns `True` on fresh pipeline (idle, no model loaded)
  - [x] 5.6 Test `is_healthy()` returns `True` after successful model load (pipe + compel set)
  - [x] 5.7 Test `is_healthy()` returns `False` on corrupted state (pipe set, compel None)
  - [x] 5.8 Test file: `tests/test_story_2_1_pipeline_lazy_loading.py`

## Dev Notes

### Architecture Decision: ADR-001 (Relevant Extract)

> **Decision**: Singleton with module-level instantiation and destructive model swap.
> **Action Items**:
> - Implement true lazy-loading: remove `self.load_model(DEFAULT_MODEL)` from `__init__`, let `generate()` handle first load via existing `self.load_model(model)` call. **One-line fix** + "model loading" state feedback to client.
> - Add `is_healthy()` internal check before each generation
> - Add `warnings.warn("Placeholder: embedding pre-computation not implemented")` in `compute_embedding()` to prevent silent placeholder usage in Growth phase
>
> [Source: _bmad-output/planning-artifacts/architecture.md#ADR-001]

### Current Code State — Critical Lines

**pipeline.py:34-51** — `__init__` currently calls `load_model(DEFAULT_MODEL)`:
```python
def __init__(self):
    if hasattr(self, '_initialized') and self._initialized:
        return
    logger.info("Initialisation du pipeline flexible")
    self.device = "cuda" if torch.cuda.is_available() else "cpu"
    self.current_model = None
    self.pipe = None
    self.compel = None
    self.loaded_loras = {}
    self.ip_adapter_loaded = False
    self.load_model(DEFAULT_MODEL)  # ← REMOVE THIS LINE
    self._initialized = True
    logger.info("Pipeline pret sur %s", self.device)
```

**pipeline.py:381** — Module-level singleton instantiation:
```python
pipeline = FlexiblePipeline()  # Currently triggers model download at import!
```
After removing `load_model()` from `__init__`, this will only initialize state variables — fast, no GPU, no downloads.

**pipeline.py:302** — `generate()` already calls `load_model()`:
```python
self.load_model(model)  # This becomes the lazy-load trigger
```

**pipeline.py:256-265** — `compute_embedding()` placeholder:
```python
def compute_embedding(self, image_path: str) -> torch.Tensor:
    return torch.zeros(1)  # Placeholder — add warnings.warn BEFORE this line
```

**worker.py:73** — Existing state update convention:
```python
self.update_state(state="PROGRESS", meta={"step": "initialisation"})
```

**worker.py:97-99** — Where to add model_loading state:
```python
# Add model_loading state BEFORE calling pipeline.generate()
self.update_state(state="PROGRESS", meta={"step": "model_loading", "progress": 5})
```

### Celery State Update Convention

From architecture: states use `self.update_state(state="PROGRESS", meta={...})`. Meta must always include `"step"` (string) and `"progress"` (int 0-100).

Standard step sequence: `"initialisation"` → `"model_loading"` → `"generation_gpu"` → `"sauvegarde"` → `"termine"`

The `"model_loading"` step is **new** — insert it between `initialisation` and `generation_gpu`.

### Project Structure Notes

- All changes in `app/pipeline.py` and `app/worker.py` — no new files needed (except test file)
- Test file: `tests/test_story_2_1_pipeline_lazy_loading.py` (follows naming convention from Sprint 1)
- Testing pattern: mock heavy ML deps in conftest.py (torch, diffusers, compel all mocked at session level)
- Existing fixtures: `app` (session), `client` (authenticated), `noauth_client`

### Testing Pattern from Previous Stories

**conftest.py mocks all ML deps** — torch, diffusers, compel, etc. are `MagicMock()` objects. This means:
- `torch.zeros(1)` will return a MagicMock, not a real tensor — test the call, not the value
- `torch.cuda.is_available()` returns a MagicMock (truthy) — device will be "cuda"
- `warnings.warn()` can be tested with `pytest.warns(UserWarning, match="Placeholder")`
- For `is_healthy()` tests: directly set `pipeline.pipe`, `pipeline.compel`, `pipeline.current_model` attributes

**Test naming convention**: `test_story_{epic}_{story}_{description}.py` (e.g., `test_story_2_1_pipeline_lazy_loading.py`)

### Previous Story Intelligence (Story 5.3)

Key learnings from Story 5.3 (logging migration):
- **Dockerfile Python 3.11 fix** was a DEVIATION — pre-existing tech debt, not related. The dev should be aware Dockerfile was modified.
- **56 tests currently passing** (47 from Sprint 1 + 9 from Story 5.3). Regressions must be zero.
- **Logger pattern**: all modules use `logger = logging.getLogger(__name__)` — already in place in pipeline.py and worker.py.
- **Code review caught**: AC tests not comprehensive enough on first pass. Ensure each AC has at least one dedicated test.

### Anti-Patterns to Avoid

1. **DO NOT** add a new config variable for lazy-loading toggle — this is always-on per ADR-001
2. **DO NOT** change the Singleton pattern — that's Epic 6 (factory pattern, Growth phase)
3. **DO NOT** implement actual embedding pre-computation — only add `warnings.warn()`, the placeholder return stays
4. **DO NOT** change `generate()` method signature — only the init behavior changes
5. **DO NOT** change `load_model()` behavior — it already handles "already loaded" check at line 56-59
6. **DO NOT** add conditional model_loading state (always emit it, even if model is cached — the step is fast when cached)
7. **DO NOT** modify any GPU memory management (gc.collect, cuda.empty_cache) — those are correct and stable

### Scope Boundaries

**In scope:**
- Remove `self.load_model(DEFAULT_MODEL)` from `__init__`
- Add `warnings.warn()` to `compute_embedding()`
- Add `is_healthy()` method
- Add `model_loading` Celery state in worker
- Tests for all 3 ACs

**Out of scope:**
- Factory pattern (Epic 6)
- Actual embedding pre-computation (Story 7.2)
- Health check enrichment (Story 4.2)
- Queue intelligence (Story 2.2 — next story)
- Any API endpoint changes

### References

- [Source: _bmad-output/planning-artifacts/architecture.md#ADR-001]
- [Source: _bmad-output/planning-artifacts/epics.md#Story-2.1]
- [Source: app/pipeline.py] (FlexiblePipeline class — lines 23-381)
- [Source: app/worker.py] (generate_image_task — lines 42-148)
- [Source: app/config.py] (config constants)
- [Source: tests/conftest.py] (test fixtures and mock setup)
- [Source: _bmad-output/implementation-artifacts/5-3-logging-migration.md] (previous story learnings)

## Dev Agent Record

### Agent Model Used

Claude Opus 4.6 (claude-opus-4-6)

### Debug Log References

None — clean implementation, no debugging required.

### Completion Notes List

1. **Task 1 — Lazy loading**: Removed `self.load_model(DEFAULT_MODEL)` from `__init__`. The `__init__` now only sets state variables. `generate()` at line 311 triggers first load via `self.load_model(model)`. Module-level `pipeline = FlexiblePipeline()` at line 390 is now instant (no model download).
2. **Task 2 — Worker state feedback**: Added `model_loading` step (progress=5) between `initialisation` and `generation_gpu` in `generate_image_task`. State sequence is now: `initialisation` → `model_loading` → `generation_gpu` → `sauvegarde` → `termine`.
3. **Task 3 — compute_embedding warning**: `warnings.warn("Placeholder: embedding pre-computation not implemented")` added before `return torch.zeros(1)`. `import warnings` was already present (added during lazy-loading changes).
4. **Task 4 — is_healthy()**: Method added at line 263-274. Returns `True` for idle state (all None) and fully loaded state (all set). Returns `False` for any partial/corrupted combination.
5. **Task 5 — Tests**: 8 tests written covering all 3 ACs. All 64 tests pass (56 existing + 8 new), zero regressions.
6. **No deviations** from story scope. No anti-patterns introduced. No new dependencies.

### File List

| File | Action | Description |
|------|--------|-------------|
| `app/pipeline.py` | Modified | Removed `load_model(DEFAULT_MODEL)` from `__init__`, added `warnings.warn()` to `compute_embedding()`, added `is_healthy()` method |
| `app/worker.py` | Modified | Added `model_loading` Celery state update before `generation_gpu` |
| `tests/test_story_2_1_pipeline_lazy_loading.py` | Created | 8 tests for AC #1 (lazy-loading), AC #2 (warning), AC #3 (is_healthy) |
