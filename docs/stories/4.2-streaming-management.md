# Story 4.2: Gestion du Streaming de réponse texte

**Status:** Approved
**Story:**
**As a** User,
**I want** to see the agent's response appear character-by-character,
**so that** the interaction feels more natural and less latent.

## Acceptance Criteria
1.  Le `LlmClient` supporte le mode `stream=True`.
2.  Le H-Core transmet chaque token reçu du LLM via le bus Redis instantanément.
3.  Utilisation d'un type de message H-Link spécifique ou d'un flag `is_partial` pour les fragments de texte.
4.  L'interface A2UI consomme ces fragments pour alimenter l'effet typewriter en continu.

## Tasks / Subtasks
- [ ] Task 1: Backend Stream Pipe (AC: 1, 2)
    - [ ] Modifier `BaseAgent` pour supporter le traitement des streams.
    - [ ] Créer une boucle asynchrone qui itère sur le générateur de tokens du LLM.
- [ ] Task 2: Redis Messaging (AC: 3)
    - [ ] Définir comment les fragments sont envoyés (ex: channel dédié ou type `narrative.chunk`).
- [ ] Task 3: Frontend Consumption (AC: 4)
    - [ ] Mettre à jour `network.js` pour gérer l'accumulation de texte.

## Dev Notes
**Technical Context from Architecture:**

*   **Responsiveness:**
    - Source: `docs/retrospectives/sprint-3-retro.md#3-plan-daction`
    - "Le backend doit supporter le streaming token par token".

## Change Log
| Date | Version | Description | Author |
| --- | --- | --- | --- |
| 2026-01-20 | 1.0 | Initial draft | Bob (SM) |
