# Story 14.4: Implémentation TTS avec MeloTTS + OpenVoice

**Status:** in-progress
**Epic:** 14 - Sensory Layer (Ears & Voice)

---

## Story

As a User,
I want agents to speak with natural, distinctive voices using MeloTTS and OpenVoice,
so that each agent has a unique vocal identity with low latency and high quality.

---

## Acceptance Criteria

1. **MeloTTS Base Integration**
   - Service TTS fonctionnel avec MeloTTS pour la synthèse vocale multilingue (FR/EN)
   - Latence < 800ms pour la première syllabe

2. **OpenVoice Cloning**
   - Intégration d'OpenVoice v2 pour le clonage vocal zero-shot
   - Chaque agent possède une voix clonée unique basée sur un échantillon de référence
   - Support de la modulation tonale (émotions, prosodie)

3. **WebSocket Streaming**
   - Audio streamé en temps réel via WebSocket vers A2UI
   - Format audio : WAV ou MP3 encodé en chunks
   - Synchronisation avec l'affichage du texte

4. **Voice Profiles**
   - Configuration par agent dans `persona.yaml` :
     - `voice_ref`: Chemin vers l'échantillon de référence
     - `speed`: Vitesse de parole (0.8 - 1.2)
     - `tone`: Modulation tonale

5. **Caching & Performance**
   - Cache Redis pour les phrases fréquentes (hash SHA-256)
   - Préchargement des modèles OpenVoice en mémoire

---

## Tasks / Subtasks

- [ ] Task 1: Setup environnement MeloTTS (Docker/container avec GPU support)
- [ ] Task 2: Intégration MeloTTS dans h-bridge (`tts_melotts.py`)
- [ ] Task 3: Setup environnement OpenVoice v2
- [ ] Task 4: Création des échantillons de référence pour Lisa, Electra, Renarde
- [ ] Task 5: Intégration OpenVoice pour le clonage (`voice_openvoice.py`)
- [ ] Task 6: Implémentation du streaming WebSocket audio
- [ ] Task 7: Mise à jour des `persona.yaml` avec les voice profiles
- [ ] Task 8: Tests de latence et qualité vocale
- [ ] Task 9: Documentation et tests E2E

---

## Dev Notes

### Technical Stack

| Composant | Technologie | Raison |
|-----------|-------------|--------|
| **TTS Base** | MeloTTS | Rapide, multilingue, open-source |
| **Voice Cloning** | OpenVoice v2 | Clonage zero-shot de qualité, modulation émotionnelle |
| **Format** | WAV 24kHz | Qualité audio optimale |
| **Serving** | FastAPI + WebSocket | Streaming temps réel |

### Architecture

```
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│   H-Core        │     │   H-Bridge       │     │   A2UI          │
│                 │     │                  │     │                 │
│  Text Response  │────▶│  TTS Service     │────▶│  Audio Playback │
│                 │     │                  │     │                 │
│                 │     │ ┌──────────────┐ │     │                 │
│                 │     │ │ MeloTTS      │ │     │                 │
│                 │     │ │ + OpenVoice  │ │     │                 │
│                 │     │ └──────────────┘ │     │                 │
└─────────────────┘     └──────────────────┘     └─────────────────┘
```

### Voice Reference Samples

- **Lisa** : Voix féminine douce, chaleureuse, légèrement maternelle
- **Electra** : Voix féminine métallique, synthétique, rapide
- **Renarde** : Voix féminine malicieuse, joueuse, mystérieuse

Les échantillons de référence (10-30s) sont stockés dans :
- `agents/{id}/media/voice_ref.wav`

### Docker Configuration

```yaml
# docker-compose.yml (à ajouter)
services:
  melotts-openvoice:
    image: melotts-openvoice:latest
    runtime: nvidia
    ports:
      - "8008:8000"
    volumes:
      - ./agents:/app/voices:ro
      - ./shared_assets/voices:/app/models
```

### Caching Strategy

- Clé Redis : `tts:cache:{hash_sha256(text+voice_id)}`
- TTL : 24h pour phrases dynamiques, 7j pour phrases statiques

---

### Project Structure Notes

**Nouveaux fichiers :**
- `apps/h-bridge/src/infrastructure/tts_melotts.py` - Client MeloTTS
- `apps/h-bridge/src/infrastructure/voice_openvoice.py` - Client OpenVoice
- `apps/h-bridge/src/services/tts_service.py` - Orchestration TTS
- `shared_assets/voices/reference_samples/` - Échantillons de référence

**Modifications :**
- `docker-compose.yml` - Ajout service melotts-openvoice
- `agents/{id}/persona.yaml` - Ajout section `voice_config`
- `apps/h-bridge/src/main.py` - Routes WebSocket audio

---

### References

- [Source: PRD V4 - Pillar 2: Deep Presence]
- [MeloTTS GitHub](https://github.com/myshell-ai/MeloTTS)
- [OpenVoice GitHub](https://github.com/myshell-ai/OpenVoice)
- [MeloTTS Paper](https://arxiv.org/abs/2402.01827)
- [OpenVoice Paper](https://arxiv.org/abs/2312.01479)

---

## Dev Agent Record

### Agent Model Used

*À compléter lors de l'implémentation*

### Debug Log References

*À compléter lors de l'implémentation*

### Completion Notes List

*À compléter lors de l'implémentation*

### File List

**Créés :**
- `apps/h-bridge/src/infrastructure/tts_melotts.py`
- `apps/h-bridge/src/infrastructure/voice_openvoice.py`
- `apps/h-bridge/src/services/tts_service.py`
- `apps/h-bridge/tests/test_tts_melotts.py`
- `apps/h-bridge/tests/test_voice_openvoice.py`

**Modifiés :**
- `docker-compose.yml`
- `agents/lisa/persona.yaml`
- `agents/electra/persona.yaml`
- `agents/renarde/persona.yaml`
- `apps/h-bridge/src/main.py`
- `apps/h-bridge/static/js/audio/playback.js`

### Change Log

- **2026-02-11** : Migration de Qwen3-TTS vers MeloTTS + OpenVoice
  - Stack plus mature et maintenue
  - Meilleur support du français
  - Clonage vocal plus simple (zero-shot)
  - Inférence plus rapide
- **2026-02-08** : Story créée initialement pour Qwen3-TTS (archivé)

---

## Notes d'Architecture (Winston)

La combinaison MeloTTS + OpenVoice offre plusieurs avantages clés :

1. **Maturité** : Les deux projets sont activement maintenus par MyShell AI
2. **Simplicité** : Pas besoin d'entraîner des LoRAs spécifiques par agent
3. **Qualité FR** : MeloTTS excelle en français (modèle dédié FR)
4. **Latence** : Inférence rapide compatible temps réel sur GPU mid-range
5. **Flexibilité** : Changement de voix instantané via échantillons de référence

**Trade-offs acceptés :**
- Dépendance à un service externe (peut tourner localement)
- Occupation mémoire GPU significative (~4-6GB pour les deux modèles)
- Pas de génération "from scratch" de voix (nécessite des échantillons)

**Prochaines étapes après cette story :**
1. Créer/recueillir les échantillons vocaux de référence
2. Benchmark temps réel avec le hardware cible
3. Intégration avec le pipeline STT (Epic 14.2/14.6)
