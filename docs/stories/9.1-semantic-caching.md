# Story 9.1: Semantic Caching & Optimization

**Status:** Done
**Epic:** 9 - Cognitive Consolidation

## Story
**As a** Developer / Platform Owner,
**I want** to cache vector embeddings for frequent text inputs using Redis,
**so that** I can reduce API costs, decrease response latency, and avoid redundant computations for repeated phrases.

## Acceptance Criteria
1. [x] A new `EmbeddingCache` component is implemented in `apps/h-core/src/infrastructure/cache.py`.
2. [x] The cache uses Redis to store mappings between hashed text and their vector embeddings.
3. [x] `LlmClient.get_embedding` is updated to check the cache before calling the external provider.
4. [x] Successful cache hits are logged as `SYSTEM_LOG` for transparency.
5. [x] Unit tests verify cache hit/miss scenarios and Redis integration.

## Tasks / Subtasks
- [ ] Backend: Implement `EmbeddingCache` (AC: #1, #2)
    - [ ] Create `cache.py` using `redis-py` (async).
    - [ ] Implement `get(text)` and `set(text, vector)` methods.
    - [ ] Use SHA-256 hashing for keys to handle long strings.
- [ ] Backend: Update `LlmClient` (AC: #3, #4)
    - [ ] Inject `EmbeddingCache` into `LlmClient`.
    - [ ] Wrap `aembedding` calls with cache logic.
    - [ ] Broadcast a `SYSTEM_LOG` when an embedding is served from cache.
- [ ] Testing: Performance & Hit Rate (AC: #5)
    - [ ] Create `apps/h-core/tests/test_cache.py`.
    - [ ] Verify that identical texts result in only ONE external API call.

## Dev Notes
- **Tech Stack**: Uses existing Redis 7.x instance. [Source: architecture/3-tech-stack.md]
- **Key Format**: `hairem:cache:emb:{sha256_hash}`.
- **TTL**: Cache entries should have a configurable TTL (default: 7 days).
- **Source Tree**: Infrastructure located in `apps/h-core/src/infrastructure/`.
- **Previous Story Context**: This builds on Story 8.3 (Vector Embeddings).

## Testing
- **Test file location**: `apps/h-core/tests/test_cache.py`
- **Framework**: `pytest`
- **Patterns**: Use `unittest.mock` for Redis dependencies.

## Change Log
| Date | Version | Description | Author |
| --- | --- | --- | --- |
| 2026-01-23 | 1.0 | Initial draft for Semantic Caching | Bob (SM) |

## Dev Agent Record
- **Agent Model Used**: Gemini 2.0 Flash
- **Debug Log References**: [N/A]
- **Completion Notes List**:
    - [Implemented EmbeddingCache with Redis and SHA-256 hashing]
    - [Integrated cache into LlmClient.get_embedding]
    - [Verified logic with new unit tests]
- **File List**:
    - `apps/h-core/src/infrastructure/cache.py`
    - `apps/h-core/src/infrastructure/llm.py`
    - `apps/h-core/src/main.py`
    - `apps/h-core/tests/test_cache.py`
    - `apps/h-core/tests/test_llm_cache.py`

## QA Results

### Review Date: 2026-01-23

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
Implementation is clean and highly effective. The use of SHA-256 for key generation ensures stability, and the normalization step (lowercase/strip) maximizes the hit rate for user inputs. The integration into `LlmClient` is appropriately guarded against Redis failures.

### Refactoring Performed
- **File**: `apps/h-core/src/main.py`
  - **Change**: Initialized `EmbeddingCache` and injected into `LlmClient`.
  - **Why**: Centralized management of caching infrastructure.

### Compliance Check
- Coding Standards: [✓]
- Project Structure: [✓]
- Testing Strategy: [✓] (Unit tests coverage for all paths)
- All ACs Met: [✓]

### Improvements Checklist
- [x] Implemented EmbeddingCache component
- [x] Integrated Redis backend
- [x] Wrapped LLM embedding calls with cache logic
- [ ] Add model name to the hash key to prevent cross-model stale data (V3.1)

### Gate Status
Gate: PASS → docs/qa/gates/9.1-semantic-caching.yml
Risk profile: docs/qa/assessments/9.1-risk-20260123.md
NFR assessment: docs/qa/assessments/9.1-nfr-20260123.md
Trace matrix: docs/qa/assessments/9.1-trace-20260123.md

### Recommended Status
[✓ Ready for Done]
