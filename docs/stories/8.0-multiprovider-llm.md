# Story 8.0: Multi-Provider LLM Integration (LiteLLM)

**Status:** Done
**Epic:** 8 - Persistent Memory & Intelligence Polishing

## Story
**As a** Developer / User,
**I want** to configure the LLM provider (Ollama, Gemini, OpenRouter, LM Studio) via environment variables,
**so that** I can easily switch between local privacy and cloud performance without code changes.

## Acceptance Criteria
1. [ ] The `LlmClient` is refactored to use `litellm` instead of the direct `openai` SDK.
2. [ ] Support for **Local Providers**:
    - `ollama`: works via `LLM_MODEL=ollama/mistral` and `LLM_BASE_URL`.
    - `lm_studio`: works via `openai/model-name` and `LLM_BASE_URL`.
3. [ ] Support for **Cloud Providers**:
    - `gemini`: works via `LLM_MODEL=gemini/gemini-1.5-flash` and `GEMINI_API_KEY`.
    - `openrouter`: works via `LLM_MODEL=openrouter/google/gemini-pro` and `OPENROUTER_API_KEY`.
4. [ ] Streaming and Tool Calling (Function Calling) remain fully functional across providers.
5. [ ] Graceful error handling is preserved (using LiteLLM's unified exception mapping).

## Tasks / Subtasks
- [ ] Backend: Update dependencies (AC: #1)
    - [ ] Add `litellm` to `pyproject.toml`.
- [ ] Backend: Refactor `LlmClient` (AC: #1, #2, #3)
    - [ ] Modify `apps/h-core/src/infrastructure/llm.py`.
    - [ ] Implement `litellm.acompletion` for both standard and streaming requests.
    - [ ] Map environment variables to LiteLLM configuration.
- [ ] Integration: Multi-provider verification (AC: #2, #3)
    - [ ] Test with a mock base URL for local providers.
    - [ ] Ensure API Keys are correctly passed for cloud providers.
- [ ] Testing: Validation (AC: #4)
    - [ ] Update `apps/h-core/tests/test_llm.py` to verify LiteLLM integration.

## Dev Notes
- **Tech Stack**: LiteLLM is already identified in the architecture as the abstraction layer. [Source: architecture/3-tech-stack.md]
- **Configuration**: Use `os.getenv` to pull `LLM_MODEL`, `LLM_API_KEY`, and `LLM_BASE_URL`.
- **Tool Calling**: LiteLLM supports OpenAI-style tool schemas and translates them for Gemini/Claude automatically.

## Testing
- Framework: `pytest`
- Mocking: Mock `litellm.acompletion` to avoid hitting real APIs during tests.

## Change Log
| Date | Version | Description | Author |
| --- | --- | --- | --- |
| 2026-01-23 | 1.0 | Initial draft for LiteLLM integration | Bob (SM) |

## QA Results

### Review Date: 2026-01-23

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
Implementation is clean and highly effective. The transition to LiteLLM provides the necessary abstraction for omnichannel expansion. The developer ensured that environment variable injection for specific providers (Gemini/OpenRouter) was handled correctly within the `LlmClient` lifecycle.

### Refactoring Performed
- **File**: `apps/h-core/src/infrastructure/llm.py`
  - **Change**: Replaced `AsyncOpenAI` with `litellm.acompletion`.
  - **Why**: To support multiple providers without branching logic.
  - **How**: Mapped `os.getenv` vars directly to `acompletion` parameters.

### Compliance Check
- Coding Standards: [✓]
- Project Structure: [✓]
- Testing Strategy: [✓] (Unit + E2E validated)
- All ACs Met: [✓]

### Improvements Checklist
- [x] Refactored to LiteLLM
- [x] Validated Gemini 2.5 response via Playwright
- [x] Updated unit tests with proper mocking
- [ ] Monitor tool calling stability across different cloud providers

### Gate Status
Gate: PASS → docs/qa/gates/8.0-multiprovider-llm.yml
Risk profile: docs/qa/assessments/8.0-risk-20260123.md
NFR assessment: docs/qa/assessments/8.0-nfr-20260123.md
Trace matrix: docs/qa/assessments/8.0-trace-20260123.md

### Recommended Status
[✓ Ready for Done]
