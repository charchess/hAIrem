# Story 4.1: Client API LLM (OpenAI-compatible)

**Status:** Approved
**Story:**
**As a** System,
**I want** a generic client capable of communicating with LLM providers (OpenAI, Ollama, Groq),
**so that** agents can generate intelligent responses based on their prompts.

## Acceptance Criteria
1.  Une classe `LlmClient` est implémentée dans `apps/h-core/src/infrastructure/llm.py`.
2.  Le client supporte les endpoints compatibles OpenAI (Base URL + API Key).
3.  Le client gère les appels asynchrones (`async/await`).
4.  Configuration dynamique via des variables d'environnement (`LLM_BASE_URL`, `LLM_API_KEY`, `LLM_MODEL`).
5.  Gestion des erreurs (Timeouts, Quota) avec logging approprié.

## Tasks / Subtasks
- [ ] Task 1: LLM Infrastructure (AC: 1, 4)
    - [ ] Ajouter `openai` et `httpx` aux dépendances Poetry.
    - [ ] Créer `apps/h-core/src/infrastructure/llm.py`.
- [ ] Task 2: Completion Logic (AC: 2, 3)
    - [ ] Implémenter la méthode `get_completion(messages, stream=False)`.
    - [ ] Gérer l'initialisation du client via les variables d'environnement.
- [ ] Task 3: Error Handling (AC: 5)
    - [ ] Implémenter des blocs try/except pour capturer les erreurs d'API.

## Dev Notes
**Technical Context from Architecture:**

*   **Provider Agnosticism:**
    - Source: `docs/project-documentation.md#3-technology-stack`
    - Doit pouvoir parler à Ollama (local) ou des APIs distantes.

*   **Asynchronicité:**
    - Utiliser `AsyncOpenAI` de la librairie officielle.

## Change Log
| Date | Version | Description | Author |
| --- | --- | --- | --- |
| 2026-01-20 | 1.0 | Initial draft | Bob (SM) |
