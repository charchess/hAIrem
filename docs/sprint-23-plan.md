# Sprint 23 â€” "L'Observatoire" Â· Monitoring, Polyphonie & Polish Final

**PÃ©riode :** Avril 2026 (semaine 3-4)  
**Objectif :** Le systÃ¨me est observable, la polyphonie visuelle est complÃ¨te, les derniers gaps UX sont fermÃ©s.

---

## Stories

### Story 23.1 â€” Monitoring Prometheus/Grafana
**PrioritÃ© :** ðŸŸ  MOYENNE (NFR-V4-01)  
**Effort :** M

**MÃ©triques cibles :**
- Latence graphe SurrealDB < 500ms (NFR-V4-01)
- Latence TTS < 800ms (NFR audio)
- Tokens consommÃ©s par agent (dÃ©jÃ  en heartbeat â†’ exporter)
- Temps de rÃ©ponse par agent

**Tests Ã  Ã©crire AVANT :**
```
apps/h-core/tests/test_metrics.py
- test_graph_query_latency_recorded()
  # Given: query SurrealDB instrumentÃ©e
  # When: query exÃ©cutÃ©e
  # Then: MetricsCollector enregistre la durÃ©e

- test_tts_latency_recorded()
- test_token_counter_increments_correctly()
- test_metrics_endpoint_returns_prometheus_format()
  # Given: GET /metrics sur h-bridge
  # When: request
  # Then: format text/plain Prometheus valide
```

**ImplÃ©mentation :**
- Ajouter `prometheus_client` aux dÃ©pendances
- Instrumenter : `SurrealDbClient._call()`, `TtsOrchestrator.synthesize()`, `LlmClient.get_completion()`
- Exposer `/metrics` dans h-bridge (FastAPI endpoint)
- `docker-compose.yml` : services `prometheus` + `grafana` avec dashboards prÃ©-configurÃ©s
- Dashboard Grafana : latences, tokens/heure, agents actifs

---

### Story 23.2 â€” Polyphonie Visuelle (Arbitration Glow)
**PrioritÃ© :** ðŸŸ  MOYENNE (Story 18.3)  
**Effort :** M

**Contexte :** Quand un agent parle, son avatar doit scale Ã  1.05 + halo lumineux. Les autres passent Ã  20% grayscale. Ce signal doit venir du backend.

**Tests Ã  Ã©crire AVANT :**
```
apps/h-core/tests/test_polyphony_signal.py
- test_speaking_agent_signal_published_on_bus()
  # Given: agent Lisa en train de gÃ©nÃ©rer sa rÃ©ponse
  # When: BaseAgent.process_message() dÃ©marre
  # Then: HLink message type=agent.speaking { agent_id: "lisa", state: "speaking" } publiÃ©

- test_speaking_signal_cleared_on_response_end()
  # Given: Lisa a fini de parler
  # When: rÃ©ponse complÃ¨te publiÃ©e
  # Then: HLink message type=agent.speaking { state: "idle" } publiÃ©

- test_only_one_agent_speaking_at_a_time()
  # Given: Lisa et Renarde toutes deux en train de rÃ©pondre
  # When: signaux publiÃ©s
  # Then: un seul agent en Ã©tat "speaking" Ã  tout moment (le premier)
```

**ImplÃ©mentation :**
- Ajouter `MessageType.AGENT_SPEAKING` dans `hlink.py`
- Publier ce signal dans `BaseAgent` au dÃ©but et Ã  la fin de `process_message()`
- Le bridge transmet au frontend (dÃ©jÃ  fait par le stream_worker)
- **Frontend (h-bridge/static)** : Ã©couter `agent.speaking` â†’ CSS scale + glow

---

### Story 23.3 â€” Flux de Discussion Inter-Agents (Story 18.4 complÃ©tion)
**PrioritÃ© :** ðŸŸ  MOYENNE  
**Effort :** S

**Contexte :** `discussion_budget = 5` existe dans l'orchestrateur mais le budget n'est pas rÃ©initialisÃ© intelligemment et le stop sur "intÃ©rÃªt qui tombe" n'est pas implÃ©mentÃ©.

**Tests :**
```
apps/h-core/tests/test_inter_agent_discussion.py
- test_discussion_stops_after_budget_exhausted()
  # Given: discussion_budget = 3
  # When: 3 Ã©changes inter-agents
  # Then: 4Ã¨me message inter-agent ignorÃ©

- test_discussion_budget_reset_on_user_message()
  # Given: budget Ã©puisÃ©
  # When: nouveau message utilisateur
  # Then: budget = MAX (5)

- test_arbiter_low_score_stops_cascade()
  # Given: rÃ©ponse inter-agent avec UTS score < 0.3 pour tous
  # When: arbiter Ã©value
  # Then: None retournÃ©, discussion s'arrÃªte naturellement
```

---

### Story 23.4 â€” DÃ©tection du Barge-in
**PrioritÃ© :** ðŸŸ¡ FAIBLE  
**Effort :** M

**Contexte :** Permettre Ã  l'utilisateur d'interrompre un agent en cours de parole.

**Tests :**
```
apps/h-core/tests/test_barge_in.py
- test_barge_in_detected_during_tts()
  # Given: TTS en cours pour Lisa
  # When: input audio dÃ©tectÃ© (wakeword ou VAD)
  # Then: SpeechQueue.interrupt() appelÃ©, TTS arrÃªtÃ©

- test_interrupted_agent_acknowledges()
  # Given: Lisa interrompue
  # When: barge-in
  # Then: Lisa publie courte rÃ©ponse d'accusÃ©
```

**ImplÃ©mentation :**
- `SpeechQueue.interrupt()` : cancelle la tÃ¢che TTS courante
- `WakewordService` Ã©met event `audio.barge_in` si audio dÃ©tectÃ© pendant TTS
- `HaremOrchestrator` souscrit Ã  cet event

---

### Story 23.5 â€” HA Discovery Automatique
**PrioritÃ© :** ðŸŸ¡ FAIBLE  
**Effort :** S

**Tests :**
```
apps/h-core/tests/test_ha_discovery.py
- test_ha_entities_fetched_on_startup()
- test_entity_list_cached_in_surrealdb()
- test_agent_tools_list_entities_from_cache()
```

**ImplÃ©mentation :**
- `HaClient.get_all_entities()` â†’ cache dans SurrealDB table `ha_entities`
- AppelÃ© au dÃ©marrage dans `_background_setup()`

---

### Story 23.6 â€” Multi-room Audio Routing
**PrioritÃ© :** ðŸŸ¡ FAIBLE  
**Effort :** L

**Tests :**
```
apps/h-core/tests/test_multiroom.py
- test_audio_routed_to_agent_room()
  # Given: Lisa dans "salon", Renarde dans "chambre"
  # When: Lisa parle
  # Then: audio broadcast uniquement vers speakers du "salon"

- test_user_location_determines_target_room()
  # Given: utilisateur dÃ©tectÃ© dans "cuisine"
  # When: message envoyÃ©
  # Then: agents de la cuisine prioritaires dans arbiter
```

**ImplÃ©mentation :**
- `LocationService.get_user_room()` via HA (prÃ©sence BT/WiFi/RFID)
- Filtrage dans `SpeechQueue` par room_id
- HA target: `media_player.salon`, `media_player.chambre`

---

### Story 23.7 â€” RedisLogHandler (rÃ©parer la rÃ©cursion)
**PrioritÃ© :** ðŸŸ¡ FAIBLE  
**Effort :** S

**Contexte :** Le `RedisLogHandler` a Ã©tÃ© dÃ©sactivÃ© (commentÃ©) pour Ã©viter une boucle infinie. Les logs ne sont plus visibles dans l'UI.

**Tests :**
```
apps/h-core/tests/test_redis_log_handler.py
- test_log_handler_does_not_recurse()
  # Given: RedisLogHandler actif
  # When: une erreur se produit pendant publish()
  # Then: pas de rÃ©cursion infinie (timeout 1s)

- test_log_handler_publishes_to_system_stream()
```

**ImplÃ©mentation :** Le `_is_emitting` guard existe dÃ©jÃ . Le bug vient probablement de l'usage de `asyncio.create_task()` dans un handler synchrone. Fix : utiliser `loop.call_soon_threadsafe()` ou une queue asyncio dÃ©diÃ©e.

---
