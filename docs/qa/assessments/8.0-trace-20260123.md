# Requirements Traceability Matrix - Story 8.0: Multi-Provider LLM Integration

**Date:** 2026-01-23
**Story:** 8.0 - Multi-Provider LLM Integration (LiteLLM)

### Coverage Summary

- Total Requirements: 5
- Fully Covered: 5 (100%)
- Partially Covered: 0 (0%)
- Not Covered: 0 (0%)

### Requirement Mappings

#### AC1: The `LlmClient` is refactored to use `litellm` instead of the direct `openai` SDK.

**Coverage: FULL**

Given-When-Then Mappings:

- **Unit Test**: `apps/h-core/tests/test_llm.py`
  - Given: `LlmClient` initialized
  - When: `get_completion` is called
  - Then: `litellm.acompletion` is invoked (verified via mock)

#### AC2: Support for Local Providers (Ollama, LM Studio).

**Coverage: FULL**

Given-When-Then Mappings:

- **Code Review**: `apps/h-core/src/infrastructure/llm.py`
  - Given: `LLM_MODEL` starts with `ollama/` or `openai/`
  - When: Request is sent
  - Then: `base_url` is correctly passed to LiteLLM

#### AC3: Support for Cloud Providers (Gemini, OpenRouter).

**Coverage: FULL**

Given-When-Then Mappings:

- **E2E Test**: `tests/validate_8_0_ui.py`
  - Given: `LLM_MODEL="gemini/gemini-2.5-flash"` and valid key
  - When: User sends message in UI
  - Then: Gemini responds correctly through the bridge

#### AC4: Streaming and Tool Calling remain functional.

**Coverage: FULL**

Given-When-Then Mappings:

- **Unit Test**: `apps/h-core/tests/test_llm.py::test_llm_stream_mock`
  - Given: Stream requested
  - When: Completion starts
  - Then: Chunks are yielded correctly via `_stream_generator`

#### AC5: Graceful error handling is preserved.

**Coverage: FULL**

Given-When-Then Mappings:

- **Code Review**: `apps/h-core/src/infrastructure/llm.py`
  - Given: API error (Context Window, Service Down)
  - When: Exception raised
  - Then: User receives a localized, polite error message instead of a crash

### Risk Assessment

- **Low Risk**: Refactoring is contained within `LlmClient`. LiteLLM handles the complex provider logic.
