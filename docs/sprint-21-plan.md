# Sprint 21 ‚Äî "La Voix" ¬∑ STT, TTS & Wakeword Complet

**P√©riode :** Mars 2026 (semaine 3-4)  
**Objectif :** Les agents entendent et parlent. Pipeline audio end-to-end fonctionnel.

---

## Contexte

Le pipeline audio est actuellement inexistant dans h-core. `VoiceModulator` g√©n√®re du SSML textuel mais rien n'est branch√© sur un vrai moteur TTS. Whisper (STT) n'est pas int√©gr√©. Le `WakewordService` existe mais est probablement un stub.

**Architecture cible :**
```
Mic ‚Üí Wakeword ‚Üí Whisper (STT) ‚Üí Message HLink ‚Üí Agents ‚Üí TTS (MeloTTS/ElevenLabs) ‚Üí Speaker
```

---

## Stories

### Story 21.1 ‚Äî Wakeword Engine (compl√©tion Story 14.2)
**Priorit√© :** üî¥ HAUTE  
**Effort :** M

**Tests √† √©crire AVANT :**
```
apps/h-core/tests/test_wakeword_complete.py
- test_wakeword_detector_initializes_without_crash()
- test_wakeword_detects_agent_name()
  # Given: buffer audio contenant "Lisa, allume la lumi√®re"
  # When: WakewordDetector.process_audio(buffer)
  # Then: event wakeword d√©clench√© avec target="lisa"

- test_wakeword_ignores_non_wakeword_audio()
- test_wakeword_publishes_to_redis_on_detection()
  # Given: wakeword d√©tect√©
  # When: handler appel√©
  # Then: message HLink publi√© sur conversation_stream avec target correct

- test_wakeword_service_lifecycle_start_stop()
```

**Impl√©mentation :**
- Compl√©ter `WakewordDetector.process_audio()` avec Vosk ou openWakeWord (l√©ger, local)
- Brancher `WakewordService` dans le d√©marrage de h-core (actuellement non d√©marr√©)
- Ajouter √† `docker-compose.yml` : service audio (acc√®s `/dev/snd` ou stream r√©seau)

**Doc :** Mettre √† jour `docs/stories/14.2-wakeword-engine.md` avec d√©cisions d'impl√©mentation.

---

### Story 21.2 ‚Äî Pipeline STT (Whisper)
**Priorit√© :** üî¥ HAUTE  
**Effort :** M

**Tests √† √©crire AVANT :**
```
apps/h-core/tests/test_stt_pipeline.py
- test_whisper_transcribes_audio_chunk()
  # Given: fichier WAV de test (fixture)
  # When: WhisperService.transcribe(audio_bytes)
  # Then: retourne string non-vide

- test_whisper_returns_empty_on_silence()
- test_stt_pipeline_publishes_hlink_on_transcription()
  # Given: transcription r√©ussie
  # When: pipeline complet
  # Then: HLinkMessage type=USER_MESSAGE sur conversation_stream

- test_stt_handles_timeout_gracefully()
- test_stt_privacy_filter_applied()
  # Given: transcription contenant un mot-cl√© secret
  # When: PrivacyFilter appliqu√©
  # Then: contenu filtr√© avant publication Redis
```

**Impl√©mentation :**
- Cr√©er `apps/h-core/src/services/audio/stt_service.py`
- Wrapper autour de `faster-whisper` (pip, inference locale GPU/CPU)
- Brancher sur le pipeline apr√®s wakeword
- `docker-compose.yml` : volume pour le mod√®le Whisper

---

### Story 21.3 ‚Äî Pipeline TTS (MeloTTS primary, ElevenLabs fallback)
**Priorit√© :** üî¥ HAUTE  
**Effort :** L

**Tests √† √©crire AVANT :**
```
apps/h-core/tests/test_tts_pipeline.py
- test_melotts_synthesizes_text()
  # Given: texte "Bonjour, comment puis-je aider ?"
  # When: MeloTtsProvider.synthesize(text, voice_id)
  # Then: retourne bytes audio non-vide

- test_tts_applies_ssml_modulation()
  # Given: texte + √©motion "joy"
  # When: VoiceModulator.apply_emotion() + TTS
  # Then: audio g√©n√©r√© avec params prosodiques corrects

- test_elevenlabs_fallback_on_melotts_timeout()
  # Given: MeloTTS timeout > 800ms
  # When: TtsOrchestrator.synthesize()
  # Then: bascule automatique sur ElevenLabs

- test_speech_queue_serializes_concurrent_requests()
  # Given: 3 agents veulent parler en m√™me temps
  # When: SpeechQueue re√ßoit les 3 requ√™tes
  # Then: elles sont jou√©es dans l'ordre (FIFO avec priorit√© user > agent)

- test_audio_chunk_broadcast_via_redis()
  # Given: synth√®se termin√©e
  # When: audio pr√™t
  # Then: publi√© en chunks base64 sur system_stream pour le bridge
```

**Impl√©mentation :**
- `apps/h-core/src/services/audio/tts_orchestrator.py` : abstraction TTS avec fallback
- `apps/h-core/src/services/audio/melotts_provider.py` : wrapper MeloTTS HTTP (Docker container)
- `apps/h-core/src/services/audio/elevenlabs_provider.py` : REST API ElevenLabs
- `apps/h-core/src/services/audio/speech_queue.py` : file FIFO avec priorit√©s
- Ajouter service `melotts` dans `docker-compose.yml`

**NFR :** TTS latence < 800ms pour phrase courte (< 20 mots). Monitorer dans heartbeat.

---

### Story 21.4 ‚Äî Neural Voice Assignment par agent
**Priorit√© :** üü† MOYENNE  
**Effort :** S

**Contexte :** `neural_voice_assignment.py` existe dans h-bridge mais n'est pas branch√© sur les agents au chargement.

**Tests :**
```
apps/h-core/tests/test_neural_voice.py
- test_each_agent_has_assigned_voice()
  # Given: Lisa, Renarde, Electra charg√©es
  # When: NeuralVoiceAssignment.get_voice(agent_id)
  # Then: voix distincte pour chaque agent

- test_voice_persists_between_sessions()
  # Given: voix assign√©e √† Lisa en DB
  # When: red√©marrage du syst√®me
  # Then: m√™me voix r√©cup√©r√©e
```

**Impl√©mentation :** Lire `voice_id` depuis `manifest.yaml` ou SurrealDB. Brancher dans `BaseAgent.__init__`.

---

### Story 21.5 ‚Äî Reconnaissance vocale par utilisateur
**Priorit√© :** üü° MOYENNE  
**Effort :** M

**Contexte :** `voice_recognition/` existe (embedding, matcher, models, repository) mais n'est pas branch√© dans le pipeline STT.

**Tests :**
```
apps/h-core/tests/test_voice_recognition_pipeline.py
- test_voice_embedding_identifies_registered_user()
- test_unknown_voice_defaults_to_anonymous()
- test_user_id_injected_in_hlink_message()
  # Given: voix reconnue comme "user_123"
  # When: HLink message cr√©√©
  # Then: payload.user_id = "user_123"
```

---

### Story 21.6 ‚Äî Documentation Audio
**Livrable :** `docs/architecture/23-audio-pipeline.md`
- Sch√©ma flux : Mic ‚Üí Wakeword ‚Üí STT ‚Üí PrivacyFilter ‚Üí Bus ‚Üí Agents ‚Üí TTS ‚Üí Queue ‚Üí Speaker
- Config Docker pour les services audio
- Variables d'environnement : `WHISPER_MODEL`, `MELOTTS_URL`, `ELEVENLABS_API_KEY`

---
