# Story 14.4: Neural TTS (MeloTTS + OpenVoice)

Status: review

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

## Story

As an Agent (e.g., Lisa),
I want to speak with a unique, expressive, and natural voice in real-time,
so that I can have fluid vocal conversations with the user that reflect my personality.

## Acceptance Criteria

1. Given the system receives text, it should generate high-quality French speech using MeloTTS
2. Given the generation process starts, it should stream audio chunks immediately for low latency (< 500ms)
3. Given an agent has a specific voice profile, the system should clone/imitate that voice using provided reference audio or embeddings
4. Given the text contains emotion tags (e.g. `[happy]`), the voice prosody/intonation should adapt accordingly
5. Given the implementation is complete, all TTS pipeline tests should pass

## Tasks / Subtasks

- [ ] Install and configure Neural TTS dependencies (AC: 1)
  - [ ] Install MeloTTS and required libraries (PyTorch, unidic, etc.)
  - [ ] Download French language models and resources
  - [ ] Configure GPU acceleration with CPU fallback
  - [ ] Verify model loading and basic synthesis
- [ ] Implement TTS Service with Streaming (AC: 2)
  - [ ] Create `TTSService` class for model management
  - [ ] Implement streaming generation (generator yielding audio chunks)
  - [ ] Integrate with WebSocket bridge for real-time transmission
  - [ ] Optimize chunk size for balance between latency and network overhead
- [ ] Implement Voice Cloning/Style Support (AC: 3)
  - [ ] Add support for speaker embeddings/reference audio
  - [ ] Implement voice profile loading from agent configuration
  - [ ] Integrate OpenVoice style adaptation (if compatible/needed)
  - [ ] Create default voice profiles for existing agents (Lisa, etc.)
- [ ] Add Prosody/Emotion Control (AC: 4)
  - [ ] Implement text pre-processor to extract tags like `[happy]`, `[sad]`
  - [ ] Map emotion tags to TTS parameters (speed, pitch, style vector)
  - [ ] Ensure natural transitions between emotional states
- [ ] Create Frontend Audio Player (AC: 2)
  - [ ] Create `audio_player.js` for gapless streaming playback
  - [ ] Handle audio queueing and synchronization
  - [ ] Add visual feedback when agent is speaking
- [ ] Create comprehensive TTS tests (AC: 5)
  - [ ] Unit tests for text processing and tag extraction
  - [ ] Integration tests for TTS generation pipeline
  - [ ] Performance tests for latency measurement
  - [ ] Verification of multi-speaker support

## Dev Notes

### Technical Context

This story implements the vocal output capability of the Sensory Layer (Epic 14). We are choosing **MeloTTS** as the primary engine for its SOTA quality/speed ratio, native French support, and streaming capabilities. We will design the architecture to support voice cloning (OpenVoice style) and emotive prosody.

### Architecture Requirements

- **Engine**: MeloTTS (primary) + OpenVoice (style/cloning layer if needed)
- **Language**: French (priority) + Multi-language support
- **Latency**: Streaming architecture is mandatory. First audio byte should arrive < 500ms after text receipt.
- **Hardware**: Optimized for GPU (CUDA) but MUST function on CPU (with higher latency).
- **Privacy**: 100% Local processing.

### Implementation Strategy

#### 1. Backend TTS Service (`handlers/tts.py`)
```python
class TTSService:
    def __init__(self):
        self.model = None
        self.device = "auto"
        
    async def initialize(self):
        from melo.api import TTS
        # Load French model
        self.model = TTS(language='FR', device=self.device)
        
    async def stream_synthesis(self, text, speaker_id, speed=1.0):
        """Yields audio chunks for streaming."""
        # Use MeloTTS streaming API if available, or chunk text by sentences
        for chunk in self.model.tts_to_file(text, speaker_id, ...):
            yield chunk
```

#### 2. Text Pre-processing
The system needs to handle emotion tags provided by the LLM:
Input: `"[happy] Oh, c'est merveilleux ! [neutral] Je suis contente pour toi."`
Logic:
1. Parse string -> `[('happy', "Oh, c'est merveilleux !"), ('neutral', "Je suis contente pour toi.")]`
2. Generate audio for segment 1 with `speed=1.1`, `pitch=high` (or style vector)
3. Generate audio for segment 2 with default params
4. Stream continuously

#### 3. Voice Profiles (`personas/lisa.yaml`)
```yaml
voice:
  engine: melotts
  model: fr_FR-camembert
  speaker_ref_path: "voices/lisa_ref.wav" # For cloning
  base_speed: 1.0
  style_map:
    happy: 
      speed: 1.1
      sdp_ratio: 0.2 # Style strength
```

### Risks & Mitigations

- **Installation Complexity**: MeloTTS has complex dependencies (dictionaries, nltk data). We need a robust setup script.
- **VRAM Usage**: Loading Whisper AND MeloTTS might saturate GPU memory. We may need model offloading/swapping logic if VRAM < 8GB.
- **Latency**: If CPU-only, streaming might be slower than real-time playback. Mitigation: Aggressive caching of common phrases.

### References

- [MeloTTS Repository](https://github.com/myshell-ai/MeloTTS)
- [OpenVoice Repository](https://github.com/myshell-ai/OpenVoice)
- [Epic 14 PRD](docs/prd/epic-14-sensory-layer.md)

## Dev Agent Record

### Agent Model Used

big-pickle (opencode/big-pickle)

### Completion Notes List

- Defined MeloTTS as the core engine for French support and speed
- Established streaming-first architecture requirement
- Planned emotion tag support for expressive speech
- Designed fallback mechanisms for hardware limitations

### File List

- `apps/h-bridge/src/handlers/tts.py` (New)
- `apps/h-bridge/static/js/audio_player.js` (New)
- `apps/h-bridge/models/tts/` (New directory)
- `requirements.txt` (Update)
- `apps/h-bridge/src/main.py` (Update for TTS routes)
