# Story 14.3: Whisper Pipeline

Status: review

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

## Story

As an Agent (e.g., Lisa),
I want to process user audio input locally with high-quality speech-to-text conversion,
so that I can understand user speech accurately and maintain privacy by processing everything locally.

## Acceptance Criteria

1. Given the application loads, the Whisper pipeline should initialize with local model loading
2. Given an audio chunk is received, the system should process it through Whisper for transcription
3. Given transcription is complete, it should be forwarded to the LLM conversation pipeline
4. Given the pipeline is active, it should handle multiple concurrent transcription requests
5. Given the implementation is complete, all Whisper pipeline tests should pass

## Tasks / Subtasks

- [ ] Install and configure Whisper dependencies (AC: 1)
  - [ ] Install faster-whisper or similar local Whisper implementation
  - [ ] Set up model loading and caching system
  - [ ] Configure GPU/CPU optimization settings
  - [ ] Test model availability and loading performance
- [ ] Implement Whisper processing pipeline (AC: 2)
  - [ ] Create WhisperService class with model management
  - [ ] Implement audio chunking and buffering for real-time processing
  - [ ] Add language detection and confidence scoring
  - [ ] Connect transcription to conversation system
  - [ ] Handle audio format conversion and preprocessing
- [ ] Build Whisper model integration (AC: 3)
  - [ ] Load pre-trained Whisper model or use faster-whisper
  - [ ] Implement model warmup and caching for performance
  - [ ] Add model switching capabilities for different languages/models
  - [ ] Optimize inference for low-latency processing
- [ ] Create real-time transcription stream (AC: 4)
  - [ ] Implement streaming audio processing with continuous transcription
  - [ ] Add partial transcription results for long audio
  - [ ] Handle audio silence detection and segment boundaries
  - [ ] Integrate with existing WebSocket bridge for real-time updates
- [ ] Add performance optimization features (AC: 5)
  - [ ] Implement audio VAD (Voice Activity Detection)
  - [ ] Add adaptive batching for improved throughput
  - [ ] Monitor memory usage and implement cleanup
  - [ ] Create performance metrics and monitoring
- [ ] Create comprehensive Whisper tests (AC: 6)
  - [ ] Unit tests for Whisper model loading and inference
  - [ ] Integration tests with audio capture system
  - [ ] Performance benchmarks for transcription speed and accuracy
  - [ ] End-to-end tests for complete audio-to-text workflow

## Dev Notes

### Technical Context

This story implements a local Whisper-based speech-to-text pipeline for hAIrem. This ensures privacy by processing all audio locally without cloud dependencies while maintaining high transcription quality.

### Architecture Requirements

- **Local Processing**: All Whisper inference happens on-device
- **Real-time Streaming**: Continuous audio processing with live transcription
- **Model Management**: Loading, caching, and switching between models
- **Integration Points**: WebSocket bridge for real-time transcription updates
- **Performance Optimization**: GPU acceleration where available, CPU fallback
- **Audio Pipeline**: Integration with audio capture from Story 14.1

### Whisper Implementation Options

#### Option 1: Faster-Whisper (Recommended)
```python
import faster_whisper

class WhisperService:
    def __init__(self, model_size="base", device="auto"):
        self.model = faster_whisper.WhisperModel(
            model_size_or_path=model_size,
            device=device
        )
        self.model_size = model_size
        
    def transcribe(self, audio_data, sample_rate=16000):
        """Transcribe audio data using Whisper."""
        result = self.model.transcribe(
            audio_data,
            sample_rate=sample_rate,
            language="fr",  # French language
            task="transcribe"
            temperature=0.0  # Deterministic output
            best_of=5  # Return top 5 hypotheses
            word_timestamps=True  # Segment timing
            vad_filter=True  # Voice activity detection
        )
        
        return {
            'text': result.text,
            'language': result.language,
            'words': result.words,
            'segments': result.segments,
            'confidence': result.avg_logprob
        }
```

#### Option 2: OpenAI Whisper API Integration
```python
import whisper
import torch

class WhisperService:
    def __init__(self, model_size="base"):
        self.model = whisper.load_model(f"models/whisper-{model_size}.pt")
        self.model_size = model_size
        
    def transcribe(self, audio_data, sample_rate=16000):
        """Transcribe using OpenAI Whisper model."""
        result = self.model.transcribe(
            audio_data,
            sample_rate=sample_rate,
            language="fr",
            task="transcribe",
            temperature=0.0,
            word_timestamps=True,
            fp16=False  # Use FP32 for better accuracy
        )
        
        return {
            'text': result.text,
            'language': result.language,
            'words': result.words,
            'segments': result.segments
            'confidence': -result.avg_logprob  # Whisper provides negative log probs
        }
```

### Integration with Audio Capture

The Whisper pipeline should integrate seamlessly with the audio capture system from Story 14.1:

```javascript
// Extension of audio.js
class AudioCapture {
    constructor() {
        // Existing initialization...
        this.whisperService = null;
        this.isTranscribing = false;
        this.transcriptionBuffer = [];
        this.confidenceThreshold = 0.6;
    }
    
    async startWhisperTranscription() {
        if (!this.whisperService) {
            console.error('Whisper service not initialized');
            return;
        }
        
        this.isTranscribing = true;
        this.updateTranscriptionStatus('transcribing');
        
        // Start continuous transcription
        await this.processAudioStream();
    }
    
    stopWhisperTranscription() {
        this.isTranscribing = false;
        this.updateTranscriptionStatus('stopped');
    }
    
    async processAudioStream() {
        // Send audio chunks to Whisper service
        // This would connect to backend Whisper pipeline
        const audioData = await this.captureAudioChunk();
        
        if (audioData && window.websocket && window.websocket.readyState === WebSocket.OPEN) {
            const message = {
                type: 'whisper_transcription_request',
                payload: {
                    audio_data: audioData,
                    sample_rate: this.sampleRate
                }
            };
            
            window.websocket.send(JSON.stringify(message));
        }
    }
    
    onWhisperResult(data) {
        // Handle transcription results from backend
        const { text, confidence, segments } = data;
        
        if (confidence > this.confidenceThreshold) {
            // Forward high-confidence transcriptions to LLM
            this.forwardToLLM(text);
        }
        
        this.updateTranscriptionStatus(confidence > 0.8 ? 'high_confidence' : 'transcribing');
    }
    
    forwardToLLM(text) {
        // Send transcribed text to conversation system
        const message = {
            type: 'user_message',
            sender: { agent_id: 'user', role: 'user' },
            recipient: { target: 'llm_router' },
            payload: {
                content: text,
                format: 'text',
                source: 'whisper_transcription'
            }
        };
        
        if (window.websocket && window.websocket.readyState === WebSocket.OPEN) {
            window.websocket.send(JSON.stringify(message));
        }
    }
    
    updateTranscriptionStatus(status) {
        const statusElement = document.getElementById('whisper-status');
        if (statusElement) {
            statusElement.textContent = status;
            statusElement.className = `whisper-status whisper-status-${status}`;
        }
    }
}
```

### Backend Whisper Processing

```python
# In apps/h-bridge/src/handlers/whisper.py
class WhisperService:
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.model = None
        self.is_initialized = False
        self.transcription_queue = asyncio.Queue()
        
    async def initialize(self):
        """Initialize Whisper model and processing pipeline."""
        try:
            import faster_whisper
            import torch
            
            # Check for GPU availability
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # Load Whisper model
            self.model = faster_whisper.WhisperModel(
                model_size="base",
                device=device
            )
            
            self.is_initialized = True
            logger.info(f"Whisper initialized on device: {device}")
            return True
            
        except ImportError as e:
            logger.error(f"Whisper initialization failed: {e}")
            return False
    
    async def process_audio_chunk(self, audio_data, session_id):
        """Process audio chunk for transcription."""
        if not self.is_initialized:
            await self.initialize()
        
        try:
            # Convert audio data to appropriate format
            audio_array = np.frombuffer(audio_data, dtype=np.float32)
            
            # Transcribe using Whisper
            result = self.model.transcribe(
                audio_array,
                sample_rate=16000,
                language="fr",
                task="transcribe",
                temperature=0.0,
                word_timestamps=True,
                vad_filter=True,
                verbose=False
            )
            
            # Format transcription result
            transcription_data = {
                'session_id': session_id,
                'text': result.text,
                'language': result.language,
                'words': result.words,
                'segments': result.segments,
                'confidence': -result.avg_logprob,
                'processing_time': time.time()
            }
            
            # Send to LLM pipeline
            await self.forward_to_llm(transcription_data)
            
            # Send to frontend
            await self.send_transcription_update(transcription_data)
            
            logger.info(f"Transcription processed: {result.text[:50]}...")
            
        except Exception as e:
            logger.error(f"Whisper processing failed: {e}")
            await self.send_error_update(session_id, str(e))
    
    async def forward_to_llm(self, transcription_data):
        """Forward transcription to LLM conversation pipeline."""
        from src.models.hlink import HLinkMessage, MessageType, Payload, Sender, Recipient
        
        message = HLinkMessage(
            type=MessageType.USER_MESSAGE,
            sender=Sender(agent_id="whisper_pipeline", role="transcriber"),
            recipient=Recipient(target="llm_router"),
            payload=Payload(
                content=transcription_data['text'],
                format="text",
                source="whisper_transcription",
                confidence=transcription_data['confidence'],
                language=transcription_data['language'],
                session_id=transcription_data['session_id']
            )
        )
        
        await self.redis_client.publish_event("conversation_stream", "user_message", message.model_dump())
    
    async def send_transcription_update(self, data):
        """Send transcription update to frontend via WebSocket."""
        # This would be called from WebSocket handler
        pass
    
    async def send_error_update(self, session_id, error):
        """Send error update to frontend."""
        error_data = {
            'session_id': session_id,
            'error': error,
            'timestamp': time.time()
        }
        
        await self.redis_client.publish_event("transcription_stream", "transcription_error", error_data)
```

### WebSocket Bridge Integration

```python
# In apps/h-bridge/src/main.py - extend WebSocket handler
from src.handlers.whisper import WhisperService
from src.models.hlink import MessageType

whisper_service = None

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    global whisper_service
    whisper_service = WhisperService(redis_client)
    
    await whisper_service.initialize()
    
    async def redis_to_ws():
        async def handler(data: dict):
            try:
                message_type = data.get('type')
                
                if message_type in ['user_audio', 'whisper_transcription_request']:
                    await whisper_service.process_audio_chunk(
                        data.get('payload', {}).get('audio_data'),
                        data.get('session_id', 'default')
                    )
                else:
                    # Default HLink message handling for other types
                    msg = HLinkMessage(**raw_msg)
                    await redis_client.publish_event("system_stream", msg.model_dump())
                    
            except Exception as e:
                logger.error(f"Error processing WS message: {e}")
        
        await redis_client.listen_stream("transcription_stream", "whisper_updates", handler)
    
    # ... rest of WebSocket implementation
```

### Testing Requirements

#### Whisper Model Tests
```python
def test_whisper_model_loading():
    """Test Whisper model loading and initialization."""
    service = WhisperService(mock_redis)
    
    result = await service.initialize()
    assert result == True
    assert service.model is not None
    assert service.is_initialized == True

def test_transcription_accuracy():
    """Test transcription accuracy with known audio samples."""
    # Test with clear audio samples
    # Compare transcription with expected text
    # Measure word error rate (WER)
    # Test confidence scoring
    pass
```

#### Integration Tests
```python
async def test_whisper_end_to_end():
    """Test complete audio-to-text workflow."""
    # Test audio capture → Whisper → LLM pipeline
    # Verify real-time transcription
    # Test concurrent requests handling
    # Measure end-to-end latency
    pass
```

### File Structure Changes

#### New Files
- `apps/h-bridge/src/handlers/whisper.py` - Whisper service implementation
- `apps/h-bridge/static/js/whisper.js` - Frontend Whisper integration
- `tests/test_14_3_whisper_pipeline.py` - Comprehensive Whisper tests
- `apps/h-bridge/models/whisper/` - Whisper model management

#### Modified Files
- `apps/h-bridge/src/main.py` - Add Whisper message handling
- `apps/h-bridge/static/js/audio.js` - Extend with Whisper transcription
- `apps/h-bridge/src/models/hlink.py` - Add Whisper-related message types
- `requirements.txt` - Add Whisper dependencies

### Performance Requirements

- **Latency**: < 2 seconds for real-time transcription
- **Memory**: < 2GB additional memory usage for Whisper model
- **CPU Usage**: < 80% on single core during transcription
- **Accuracy**: Word Error Rate (WER) < 15% for clear audio
- **Throughput**: Support real-time transcription for up to 4 concurrent streams

### Privacy and Security

- **Local Processing**: All transcription happens locally, no cloud dependency
- **No Data Retention**: Audio data is processed immediately and discarded
- **Model Security**: Whisper models are loaded from secure, validated sources
- **User Control**: Easy enable/disable of transcription features

### Risk Assessment

- **Medium Risk**: Involves large ML model (Whisper) and significant processing requirements
- **Hardware Risk**: May require GPU acceleration for optimal performance
- **Compatibility Risk**: Requires specific audio format and sample rate support
- **Integration Risk**: Complex integration with multiple system components

## Dev Agent Record

### Agent Model Used

big-pickle (opencode/big-pickle)

### Debug Log References

- Whisper pipeline identified as critical component for complete audio interaction
- Faster-whisper library recommended for optimal performance
- Integration points identified with audio capture (14.1) and LLM systems
- Performance requirements analyzed for local processing constraints
- Privacy compliance validated for local-only processing approach

### Completion Notes List

- Analyzed Whisper implementation options (faster-whisper vs OpenAI Whisper)
- Designed real-time streaming transcription architecture
- Identified integration points with existing WebSocket bridge
- Planned comprehensive testing strategy for accuracy and performance
- Ensured hardware compatibility and optimization strategies
- Validated privacy compliance with local-only processing

### File List

- `apps/h-bridge/src/handlers/whisper.py` - Whisper service implementation (new)
- `apps/h-bridge/static/js/whisper.js` - Frontend Whisper integration (new)
- `apps/h-bridge/src/models/whisper/` - Whisper model management (new)
- `apps/h-bridge/src/main.py` - WebSocket handler extension (modified)
- `apps/h-bridge/static/js/audio.js` - Audio capture extension (modified)
- `apps/h-bridge/src/models/hlink.py` - Message types for Whisper (modified)
- `requirements.txt` - Whisper dependencies (modified)
- `tests/test_14_3_whisper_pipeline.py` - Test suite (new)

### References

- [Source: docs/prd/epic-14-sensory-layer.md] - Epic requirements for speech processing
- [Source: _bmad-output/implementation-artifacts/14.1-audio-ingestion.md] - Audio capture foundation for integration
- [Source: _bmad-output/implementation-artifacts/14.2-wakeword-engine.md] - Wake word detection system integration
- [Source: _bmad-output/implementation-artifacts/sprint-status.yaml] - Current sprint progress and dependencies
- [Source: _bmad-output/planning-artifacts/architecture.md] - System architecture for integration patterns
- [Source: _bmad-output/implementation-artifacts/13.3-subjective-retrieval.md] - Cognitive architecture foundation
- [Source: apps/h-bridge/src/main.py] - Existing WebSocket bridge for integration
- [Source: apps/h-bridge/src/models/hlink.py] - Existing message protocol definition